{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "384f19d0",
      "metadata": {
        "id": "384f19d0"
      },
      "source": [
        "## What is out main Aim ?\n",
        "> As humans we just want to use machines(computer) in order to redure the human effort and here our Aim for NLTk is to use the Natural language Processing to do judge/analysis of some writtend dwon text, in  CSE anaology we are using NLP to revice some sort of ouput/result of what text means and implies.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "881ff7ce",
      "metadata": {
        "id": "881ff7ce",
        "outputId": "18893c70-de84-44ea-8e66-b603a54823a6"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\socket.py:832\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    831\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 832\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[0;32m    833\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
            "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\user\\Desktop\\Desktop\\Python\\Intership\\NLTK - intership.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Desktop/Python/Intership/NLTK%20-%20intership.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Desktop/Python/Intership/NLTK%20-%20intership.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m nltk\u001b[39m.\u001b[39;49mdownload()\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py:763\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[39mif\u001b[39;00m download_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_dir \u001b[39m=\u001b[39m download_dir\n\u001b[1;32m--> 763\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interactive_download()\n\u001b[0;32m    764\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    767\u001b[0m     \u001b[39m# Define a helper function for displaying output:\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py:1113\u001b[0m, in \u001b[0;36mDownloader._interactive_download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1111\u001b[0m \u001b[39mif\u001b[39;00m TKINTER:\n\u001b[0;32m   1112\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1113\u001b[0m         DownloaderGUI(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39mmainloop()\n\u001b[0;32m   1114\u001b[0m     \u001b[39mexcept\u001b[39;00m TclError:\n\u001b[0;32m   1115\u001b[0m         DownloaderShell(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mrun()\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py:1410\u001b[0m, in \u001b[0;36mDownloaderGUI.__init__\u001b[1;34m(self, dataserver, use_threads)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_menu()\n\u001b[0;32m   1409\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1410\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fill_table()\n\u001b[0;32m   1411\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1412\u001b[0m     showerror(\u001b[39m\"\u001b[39m\u001b[39mError reading from server\u001b[39m\u001b[39m\"\u001b[39m, e)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py:1742\u001b[0m, in \u001b[0;36mDownloaderGUI._fill_table\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1740\u001b[0m     items \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ds\u001b[39m.\u001b[39mmodels()\n\u001b[0;32m   1741\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tab \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcollections\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 1742\u001b[0m     items \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ds\u001b[39m.\u001b[39;49mcollections()\n\u001b[0;32m   1743\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1744\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbad tab value \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tab\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py:594\u001b[0m, in \u001b[0;36mDownloader.collections\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollections\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 594\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_index()\n\u001b[0;32m    595\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_collections\u001b[39m.\u001b[39mvalues()\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\downloader.py:952\u001b[0m, in \u001b[0;36mDownloader._update_index\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url \u001b[39m=\u001b[39m url \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url\n\u001b[0;32m    950\u001b[0m \u001b[39m# Download the index file.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39minternals\u001b[39m.\u001b[39mElementWrapper(\n\u001b[1;32m--> 952\u001b[0m     ElementTree\u001b[39m.\u001b[39mparse(urlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_url))\u001b[39m.\u001b[39mgetroot()\n\u001b[0;32m    953\u001b[0m )\n\u001b[0;32m    954\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_timestamp \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    956\u001b[0m \u001b[39m# Build a dictionary of packages.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py:214\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[1;32m--> 214\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39;49mopen(url, data, timeout)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py:517\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    514\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[0;32m    516\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[1;32m--> 517\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(req, data)\n\u001b[0;32m    519\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[0;32m    520\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py:534\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m    533\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[1;32m--> 534\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_chain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_open, protocol, protocol \u001b[39m+\u001b[39;49m\n\u001b[0;32m    535\u001b[0m                           \u001b[39m'\u001b[39;49m\u001b[39m_open\u001b[39;49m\u001b[39m'\u001b[39;49m, req)\n\u001b[0;32m    536\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[0;32m    537\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py:494\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[0;32m    493\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 494\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    495\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py:1389\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1388\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttps_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_open(http\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mHTTPSConnection, req,\n\u001b[0;32m   1390\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context, check_hostname\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_hostname)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py:1346\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1345\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1346\u001b[0m         h\u001b[39m.\u001b[39;49mrequest(req\u001b[39m.\u001b[39;49mget_method(), req\u001b[39m.\u001b[39;49mselector, req\u001b[39m.\u001b[39;49mdata, headers,\n\u001b[0;32m   1347\u001b[0m                   encode_chunked\u001b[39m=\u001b[39;49mreq\u001b[39m.\u001b[39;49mhas_header(\u001b[39m'\u001b[39;49m\u001b[39mTransfer-encoding\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m   1348\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\http\\client.py:1285\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\u001b[39mself\u001b[39m, method, url, body\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, headers\u001b[39m=\u001b[39m{}, \u001b[39m*\u001b[39m,\n\u001b[0;32m   1283\u001b[0m             encode_chunked\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   1284\u001b[0m     \u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1285\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\http\\client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(body, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1328\u001b[0m     \u001b[39m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m     \u001b[39m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1331\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\http\\client.py:1280\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1279\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1280\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\http\\client.py:1040\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1038\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer)\n\u001b[0;32m   1039\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1040\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[0;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m \n\u001b[0;32m   1044\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n\u001b[0;32m   1045\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(message_body, \u001b[39m'\u001b[39m\u001b[39mread\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m   1046\u001b[0m         \u001b[39m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m         \u001b[39m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m         \u001b[39m# files to be taken into account.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\http\\client.py:980\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    979\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[1;32m--> 980\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m    981\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m         \u001b[39mraise\u001b[39;00m NotConnected()\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\http\\client.py:1447\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1445\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mConnect to a host on a given (SSL) port.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1447\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1449\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host:\n\u001b[0;32m   1450\u001b[0m         server_hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tunnel_host\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\http\\client.py:946\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    945\u001b[0m     \u001b[39m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 946\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection(\n\u001b[0;32m    947\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhost,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address)\n\u001b[0;32m    948\u001b[0m     \u001b[39m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n\u001b[0;32m    949\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\socket.py:832\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[39mif\u001b[39;00m source_address:\n\u001b[0;32m    831\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 832\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[0;32m    833\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[0;32m    834\u001b[0m err \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "001bd175",
      "metadata": {
        "id": "001bd175",
        "outputId": "589e6756-66d4-4b67-dce7-3056d62fa730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
        "# print(sent_tokenize(EXAMPLE_TEXT))\n",
        "print(word_tokenize(EXAMPLE_TEXT))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bd31ef1",
      "metadata": {
        "id": "5bd31ef1"
      },
      "source": [
        "For human the task of analysis/judge is done quite easily but it is impossible for computer to undertstand something which is not in in 0 or 1 (binary/macine code) \n",
        "\n",
        "\n",
        "Bammer : humans are also not good at this task. The human memory is work as electrical signal in the form of neural groups which fires in the patterns.\n",
        "\n",
        "\n",
        "As I earlier said computer are created to just reduce the human effort and in order to build there efficiency and accuracy they are limited to the some extent, here limitation means they are limited with the way they perform there task(the procedure to done something), it include .\n",
        "computers only work in binary 0,1.\n",
        "\n",
        "There are some way to convert words to values, in numbers or in signal patterns.\n",
        "\n",
        "## Pre-Processing \n",
        "> The process of converting data to something a computer can understand is reffered as pre-processing.(Big Task)\n",
        "\n",
        "- here we are going to filter out useless data, it is a major form of Pre-processing.\n",
        "- and useless words(data), are referred as 'stop words'.\n",
        "\n",
        "Example containing stop(useless) words : \n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "\n",
        "from above written example we can recognize that some words carry more meaning than other words. \n",
        " \n",
        "In real world example is, when we speak 'umm' or 'uhh'. This kind of words means nothing to our real meaning of what we say.\n",
        "we call this kind of words \"stop words\".\n",
        "\n",
        "\n",
        "\n",
        "As humans we don't have much problem to have them in our text effort lessly but machine can't do the same, it will increase the space and process time for treating/converting them as meaning words.\n",
        "\n",
        "\n",
        "\n",
        "To see list what we considered as stop word \n",
        "use following code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa447d49",
      "metadata": {
        "id": "fa447d49",
        "outputId": "2d6aa849-d309-4c1b-a550-8738fb1eafd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e25a6f4",
      "metadata": {
        "id": "7e25a6f4",
        "outputId": "7572035b-a0f1-4d5c-80bc-d5ff89b71de4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords as sp\n",
        "from nltk.tokenize import word_tokenize as wt\n",
        "\n",
        "# taking the old example_sent\n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "\n",
        "# taking the stored stop words\n",
        "stop_words = set(sp.words('english'))\n",
        "\n",
        "# tokenizing the example_sentence for filtering\n",
        "word_token = wt(example_sent)\n",
        "\n",
        "# taking out the words which are not in the sto_word set\n",
        "filtered_sentence = [ w for w in word_token if not w in stop_words]\n",
        "print(filtered_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edd36091",
      "metadata": {
        "id": "edd36091"
      },
      "source": [
        "## Steaming \n",
        "> - Steaming is technique to extract the base form of the words by **removing affixes** from the resulting words.\n",
        ">\n",
        "> - Steaming is ofter a shorter word having the **same root** meaning.\n",
        ">\n",
        "> - in other words it is sort of normalizing method to shorten the sentences, in order to increase redundancy and inefficiecy \n",
        "#### Example\n",
        "> \"I was taking a ride in the car.\"\n",
        ">\n",
        "> \"I Was riging in the car.\"\n",
        "#### Algorithm \n",
        "+ There are many steam algorithm. We will use the Porter Stemmer algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf8d3b25",
      "metadata": {
        "id": "cf8d3b25",
        "outputId": "5927151a-8fd0-4827-837e-d2928318f497"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "it\n",
            "is\n",
            "import\n",
            "to\n",
            "by\n",
            "veri\n",
            "pythonli\n",
            "while\n",
            "you\n",
            "are\n",
            "python\n",
            "with\n",
            "python\n",
            ".\n",
            "all\n",
            "python\n",
            "have\n",
            "python\n",
            "poorli\n",
            "at\n",
            "least\n",
            "onc\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "# importing the PorterStemmer algorithm package\n",
        "from nltk.stem import PorterStemmer\n",
        "# importing the sentence and word tokenizer for tokenizing \n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# create the object of PorterStemmer\n",
        "ps = PorterStemmer()  \n",
        "    \n",
        "# example sentencs\n",
        "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
        "\n",
        "# passing the example sentence in tokenizer\n",
        "words = word_tokenize(new_text)\n",
        "\n",
        "# printing the each word after steaming \n",
        "for w in words:\n",
        "    print(ps.stem(w))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b53a9a7",
      "metadata": {
        "id": "7b53a9a7"
      },
      "source": [
        "### Parts of Speech Tagging\n",
        "Parts of speech this is another kind of tokenizer, and it tokentize by tagging the part of sppech of words in sentences, tagged as verb, adjective, noun , etc.\n",
        "\n",
        "In last chapter we have only used a sample of sentence but this also happen that the sentencs or sample data you want to use is different. \n",
        "\n",
        "May the data you want to use is the chat between a customer and seller,between teacher and student. The context, grammer in both cases it different. So to make the we should have a method to train the model on the grammer/context we want to test.\n",
        "\n",
        "We are using the \"PunktSentenceTokenizer\" this is capable to learn the tagging task according the given data because it is based on the unsupervised learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94ea916b",
      "metadata": {
        "id": "94ea916b",
        "outputId": "ed5157e9-a737-4af3-c14e-3262c3957b58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
            "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
            "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
            "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
            "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "# importing state_union it is a corpus reader in PlainTextCorpusReader\n",
        "from nltk.corpus import state_union\n",
        "\n",
        "# importing the punktsentencetokenizer\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "\n",
        "# taking two type of text data set for training and testing \n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "\n",
        "# training the object of punktsentencetokenizer with train_text data set\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "\n",
        "# testing it on the sample data set\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "\n",
        "# function to print the pos with its words\n",
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized[:5]:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            print(tagged)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "\n",
        "process_content()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1875150a",
      "metadata": {
        "id": "1875150a"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            # creating the chunk of the all adverb as chunkgram\n",
        "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "            chunkParser = nltk.RegexpParser(chunkGram)\n",
        "            chunked = chunkParser.parse(tagged)\n",
        "            chunked.draw()     \n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "process_content()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c83b864a",
      "metadata": {
        "id": "c83b864a",
        "outputId": "c85ce8cc-0e0b-446f-eb89-c179fd46666c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(['(', 'dimension', 'films', ',', '\"', 'scream', '2', '\"', \"'\", 's', 'distributor', ',', 'has', 'asked', 'press', 'to', 'say', 'extremely', 'little', '--', 'if', 'anything', '--', 'about', 'the', 'film', \"'\", 's', 'twisty', 'plot', '.', 'that', \"'\", 's', 'no', 'easy', 'task', 'considering', 'the', 'wit', 'that', 'deserves', 'to', 'be', 'mentioned', 'here', ',', 'but', 'i', 'will', 'do', 'my', 'best', 'to', 'be', 'vague', '(', 'now', ',', 'there', \"'\", 's', 'a', 'first', ')', '.', ')', '\"', 'the', 'first', 'one', 'was', '[', 'good', ']', ',', 'but', 'all', 'the', 'rest', 'sucked', ',', '\"', 'said', 'a', 'cinematically', '-', 'savvy', 'teen', 'in', 'last', 'winter', \"'\", 's', 'wes', 'craven', 'thriller', '\"', 'scream', ',', '\"', 'her', 'statement', 'referring', 'to', 'the', 'films', 'of', 'the', '\"', 'nightmare', 'on', 'elm', 'street', '\"', 'series', 'but', 'really', 'putting', 'down', 'franchise', 'overkill', 'in', 'general', '.', 'the', 'comment', 'certainly', 'carries', 'clout', ':', 'for', 'every', 'truly', 'great', 'sequel', ',', 'there', 'appears', 'to', 'be', 'a', 'couple', 'of', 'duds', ',', 'making', 'one', 'wonder', 'if', 'writers', 'are', 'better', 'off', 'sticking', 'solely', 'with', 'fresh', 'ideas', '.', 'but', 'like', 'it', 'or', 'not', ',', 'along', 'comes', '\"', 'scream', '2', '.', '\"', 'and', 'believe', 'it', 'or', 'not', ',', 'it', \"'\", 's', 'a', 'doozy', '--', 'a', 'slick', ',', 'sinister', ',', 'madly', 'subversive', 'good', 'time', 'at', 'the', 'movies', ',', 'as', 'intent', 'on', 'sending', 'up', 'hollywood', \"'\", 's', 'sequel', 'syndrome', 'as', 'much', 'as', 'its', 'prequel', 'poked', 'fun', 'at', 'slasher', 'conventions', '.', '\"', 'scream', '2', '\"', 'is', 'definitely', 'that', 'rare', 'movie', 'thing', '--', 'a', 'follow', '-', 'up', 'that', 'can', 'stand', 'along', 'side', 'its', 'original', 'with', 'pride', '.', 'it', \"'\", 's', 'been', 'two', 'years', 'since', 'a', 'pair', 'of', 'overzealous', 'horror', 'movie', 'fans', 'clad', 'in', 'edvard', 'munch', '-', 'esque', 'get', '-', 'ups', 'carved', 'their', 'way', 'through', 'the', 'young', 'populace', 'of', 'woodsboro', ',', 'california', '.', 'those', 'surviving', 'the', 'ordeal', 'have', 'gotten', 'on', 'with', 'their', 'lives', '.', 'plucky', 'heroine', 'sidney', 'prescott', '(', 'neve', 'campbell', ')', 'is', 'a', 'drama', 'student', 'at', 'the', 'midwestern', 'windsor', 'college', ';', 'her', 'pop', 'culture', '-', 'whiz', 'pal', 'randy', 'meeks', '(', 'jamie', 'kennedy', ')', 'has', 'tagged', 'along', '.', 'trash', 'tabloid', 'reporter', 'gale', 'weathers', '(', 'courteney', 'cox', ')', 'has', 'written', 'a', 'best', '-', 'seller', 'based', 'on', 'their', 'ordeal', ',', 'the', 'basis', 'of', 'which', 'has', 'been', 'turned', 'into', 'a', '(', 'very', 'bad', ')', 'movie', 'called', '\"', 'stab', '.', '\"', 'and', 'dewey', 'riley', '(', 'david', 'arquette', ')', ',', 'still', 'suffering', 'from', 'wounds', 'inflicted', 'during', '\"', 'scream', ',', '\"', 'has', 'left', 'his', 'job', 'as', 'a', 'police', 'officer', 'for', 'a', 'while', '.', 'life', 'is', 'tranquil', '.', '.', '.', 'at', 'least', 'for', 'a', 'while', '.', 'several', 'sudden', 'murders', 'bring', 'sidney', ',', 'randy', ',', 'gale', 'and', 'dewey', 'together', 'again', ',', 'but', 'with', 'suspicious', 'eyes', 'cast', 'on', 'each', 'other', 'and', 'most', 'of', 'those', 'in', 'their', 'surroundings', '--', 'if', 'these', 'four', 'people', 'learned', 'anything', 'from', 'the', 'past', ',', 'it', \"'\", 's', 'to', 'trust', 'no', 'one', '.', 'thus', ',', 'the', 'possible', 'victim', '/', 'potential', 'killer', 'list', 'includes', 'said', 'quartet', ',', 'as', 'well', 'as', ':', 'derek', '(', 'jerry', 'o', \"'\", 'connell', ')', ',', 'sidney', \"'\", 's', 'new', 'beau', ';', 'cici', '(', 'sara', 'michelle', 'gellar', ')', ',', 'a', 'chatty', 'sorority', 'gal', ';', 'joel', '(', 'duane', 'martin', ')', ',', 'gale', \"'\", 's', 'cameraman', 'who', \"'\", 's', 'not', 'too', 'thrilled', 'with', 'her', 'blood', '-', 'soaked', 'past', ';', 'hallie', '(', 'elise', 'neal', ')', ',', 'sidney', \"'\", 's', 'sassy', 'roommate', ';', 'debbie', '(', 'laurie', 'metcalf', ')', ',', 'a', 'local', 'reporter', 'who', 'gives', 'gale', 'some', 'not', '-', 'too', '-', 'friendly', 'competition', ';', 'and', 'mickey', '(', 'timothy', 'olyphant', ')', ',', 'randy', \"'\", 's', 'good', 'friend', 'and', 'fellow', 'film', 'student', '.', 'cotton', 'weary', '(', 'liev', 'schreiber', ')', ',', 'the', 'man', 'sidney', 'wrongly', 'accused', 'of', 'her', 'mother', \"'\", 's', 'murder', 'in', 'scream', ',', 'also', 'shows', 'up', 'on', 'campus', '--', 'but', 'why', '?', 'like', 'the', 'first', 'scream', ',', 'craven', 'and', 'screenwriter', 'kevin', 'williamson', 'inaugurate', 'things', 'with', 'a', 'bang', '.', 'this', 'time', ',', 'it', \"'\", 's', 'an', 'extended', 'sequence', 'that', 'finds', 'an', 'african', '-', 'american', 'couple', '(', 'jada', 'pinkett', 'and', 'omar', 'epps', ')', 'attending', 'a', 'sneak', 'preview', 'of', '\"', 'stab', ',', '\"', 'making', 'sly', 'references', 'about', 'everything', 'from', 'sandra', 'bullock', 'to', 'the', 'lunchmeat', 'non', '-', 'roles', 'of', 'blacks', 'in', 'slasher', 'cinema', '.', 'before', 'tragedy', 'ensues', 'at', 'the', 'screening', ',', 'the', 'movie', '-', 'within', '-', 'a', '-', 'movie', 'setup', 'allows', 'for', 'plenty', 'of', 'jokey', 'moments', ',', 'including', 'a', 'scene', 'from', '\"', 'stab', '\"', 'which', 'turns', 'scream', \"'\", 's', 'now', '-', 'famous', 'drew', 'barrymore', 'prologue', 'on', 'its', 'ear', '.', '(', '\"', 'b', '----', ',', 'hang', 'up', 'the', 'phone', 'and', 'star', '-', '69', 'his', 'a', '--', '!', '\"', 'pinkett', 'yells', 'at', 'the', 'screen', '.', ')', 'it', \"'\", 's', 'funny', ',', 'creepy', 'stuff', '(', 'i', 'don', \"'\", 't', 'think', 'i', \"'\", 'll', 'ever', 'feel', 'safe', 'in', 'a', 'movie', 'theater', 'again', ')', 'that', 'effectively', 'foreshadows', 'the', 'ratio', 'of', 'smart', 'scares', 'and', 'spoofy', 'laughs', 'down', 'the', 'road', '.', 'another', 'flawless', 'example', 'of', 'this', 'is', 'a', 'riotous', 'cell', 'phone', '-', 'assault', 'of', 'the', 'killer', 'on', 'randy', ',', 'dewey', 'and', 'gale', 'in', 'broad', 'daylight', ';', 'it', 'builds', 'comedically', 'and', 'ends', 'with', 'genuine', 'terror', 'because', 'of', 'the', 'emotional', 'investment', 'made', 'on', 'these', 'delicious', 'characters', '.', 'there', 'are', 'other', 'superb', 'set', 'pieces', ',', 'but', 'explaining', 'them', 'risks', 'ruining', 'their', 'effect', '.', 'there', 'are', 'more', 'players', 'this', 'time', 'around', 'as', 'to', 'provide', 'both', 'a', 'higher', 'number', 'suspects', 'and', 'a', 'staggering', 'body', 'count', '.', 'it', \"'\", 's', 'something', 'of', 'a', 'trade', '-', 'in', '--', 'the', 'new', 'characters', 'are', 'too', 'large', 'in', 'number', 'to', 'be', 'as', 'fully', '-', 'developed', 'as', 'the', 'original', '\"', 'scream', '\"', 'gang', ',', 'but', 'this', 'movie', 'packs', 'in', 'knowing', 'performances', '(', 'especially', 'by', 'kennedy', 'and', 'cox', ')', 'and', 'more', 'death', 'for', 'your', 'dollar', '.', 'it', 'can', 'be', 'argued', 'that', ',', 'while', 'the', 'original', 'might', 'outclass', 'it', 'by', 'a', 'tad', ',', 'scream', '2', 'is', 'both', 'scarier', 'and', 'funnier', '.', 'it', 'certainly', 'doesn', \"'\", 't', 'skimp', 'with', 'shocks', '--', 'a', 'major', 'one', 'being', 'the', 'killing', 'of', 'one', 'cast', 'member', 'very', 'near', 'and', 'dear', 'to', 'my', 'heart', '(', 'i', 'actually', 'questioned', 'craven', 'and', 'williamson', \"'\", 's', 'judgment', 'here', ',', 'considering', 'how', 'much', 'this', 'person', 'has', 'added', 'to', 'these', 'films', ')', '.', 'sometimes', 'you', 'wish', 'that', 'more', 'could', 'have', 'been', 'done', 'with', 'stab', '--', 'the', 'two', 'scenes', 'we', \"'\", 're', 'shown', 'are', 'dead', '-', 'on', '--', 'but', 'for', 'film', 'fans', ',', 'there', 'are', 'plenty', 'of', 'subtle', 'nods', 'to', 'movies', 'like', '\"', 'the', 'usual', 'suspects', ',', '\"', '\"', 'aliens', '\"', 'and', '\"', 'the', 'empire', 'strikes', 'back', '.', '\"', 'still', ',', '\"', 'scream', '2', '\"', 'does', 'so', 'many', 'things', 'right', ',', 'it', \"'\", 's', 'petty', 'to', 'quibble', '.', 'its', 'penchant', 'for', 'parody', 'is', 'irresistible', ',', 'discussions', 'of', 'the', 'merits', 'of', 'film', 'sequels', 'and', 'all', '.', 'the', 'attractive', ',', 'solid', 'cast', 'is', 'a', 'definite', 'bonus', '.', 'and', 'the', 'denouement', '(', 'while', 'not', 'as', 'shocking', 'as', '\"', 'scream', '\"', \"'\", 's', ')', 'is', 'priceless', ';', 'the', 'film', \"'\", 's', 'final', 'revelation', 'incorporates', 'bits', 'of', 'a', 'particular', 'horror', 'film', 'prominently', 'referred', 'to', 'in', '\"', 'scream', '\"', 'with', 'one', 'of', 'the', 'best', 'bad', 'guy', 'motives', 'ever', 'put', 'on', 'paper', '.', 'all', 'this', 'and', 'a', '\"', 'scream', '3', '\"', 'is', 'in', 'the', 'works', '.', 'knowing', 'the', 'series', \"'\", 'tendency', 'for', 'self', '-', 'reference', ',', 'perhaps', 'one', 'of', 'its', 'characters', 'will', 'again', 'discuss', 'the', 'concept', 'of', 'sequel', 'justice', '.', 'mentioning', '\"', 'scream', '2', '\"', \"'\", 'll', 'help', 'them', 'in', 'their', 'argument', '.'], 'pos')\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import movie_reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36c0ef2f",
      "metadata": {
        "id": "36c0ef2f"
      },
      "outputs": [],
      "source": [
        "\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "for i in movie_reviews.fileids():\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a149b35",
      "metadata": {
        "id": "0a149b35"
      },
      "outputs": [],
      "source": [
        "random.shuffle(documents)\n",
        "\n",
        "print(documents[1])\n",
        "\n",
        "# all_words = []\n",
        "# for w in movie_reviews.words():\n",
        "#     all_words.append(w.lower())\n",
        "\n",
        "# all_words = nltk.FreqDist(all_words)\n",
        "# print(all_words.most_common(15))\n",
        "# print(all_words[\"stupid\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49dc6382",
      "metadata": {
        "id": "49dc6382"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34530d56",
      "metadata": {
        "id": "34530d56"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}