{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "384f19d0",
   "metadata": {
    "id": "384f19d0"
   },
   "source": [
    "## What is our Aim ?\n",
    "> As humans we just want to use machines in order to reduce human effort. Here, our aim with NLTK is to use Natural language Processing to do analyze some of the text. In CSE analogy, we are using NLP to receive some sort of ouput/result of what the text means and implies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881ff7ce",
   "metadata": {
    "id": "881ff7ce",
    "outputId": "18893c70-de84-44ea-8e66-b603a54823a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001bd175",
   "metadata": {
    "id": "001bd175",
    "outputId": "589e6756-66d4-4b67-dce7-3056d62fa730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
    "# print(sent_tokenize(EXAMPLE_TEXT))\n",
    "print(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd31ef1",
   "metadata": {
    "id": "5bd31ef1"
   },
   "source": [
    "For human the task of analysis/judge is done quite easily but it is impossible for computer to undertstand something which is not in in 0 or 1 (binary/macine code) \n",
    "\n",
    "\n",
    "Bammer : humans are also not good at this task. The human memory is work as electrical signal in the form of neural groups which fires in the patterns.\n",
    "\n",
    "\n",
    "As I earlier said computer are created to just reduce the human effort and in order to build there efficiency and accuracy they are limited to the some extent, here limitation means they are limited with the way they perform there task(the procedure to done something), it include .\n",
    "computers only work in binary 0,1.\n",
    "\n",
    "There are some way to convert words to values, in numbers or in signal patterns.\n",
    "\n",
    "## Pre-Processing \n",
    "> The process of converting data to something a computer can understand is reffered as pre-processing.(Big Task)\n",
    "\n",
    "- here we are going to filter out useless data, it is a major form of Pre-processing.\n",
    "- and useless words(data), are referred as 'stop words'.\n",
    "\n",
    "Example containing stop(useless) words : \n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "from above written example we can recognize that some words carry more meaning than other words. \n",
    " \n",
    "In real world example is, when we speak 'umm' or 'uhh'. This kind of words means nothing to our real meaning of what we say.\n",
    "we call this kind of words \"stop words\".\n",
    "\n",
    "\n",
    "\n",
    "As humans we don't have much problem to have them in our text effort lessly but machine can't do the same, it will increase the space and process time for treating/converting them as meaning words.\n",
    "\n",
    "\n",
    "\n",
    "To see list what we considered as stop word \n",
    "use following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa447d49",
   "metadata": {
    "id": "fa447d49",
    "outputId": "2d6aa849-d309-4c1b-a550-8738fb1eafd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25a6f4",
   "metadata": {
    "id": "7e25a6f4",
    "outputId": "7572035b-a0f1-4d5c-80bc-d5ff89b71de4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords as sp\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "\n",
    "# taking the old example_sent\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "# taking the stored stop words\n",
    "stop_words = set(sp.words('english'))\n",
    "\n",
    "# tokenizing the example_sentence for filtering\n",
    "word_token = wt(example_sent)\n",
    "\n",
    "# taking out the words which are not in the sto_word set\n",
    "filtered_sentence = [ w for w in word_token if not w in stop_words]\n",
    "print(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd36091",
   "metadata": {
    "id": "edd36091"
   },
   "source": [
    "## Steaming \n",
    "> - Steaming is technique to extract the base form of the words by **removing affixes** from the resulting words.\n",
    ">\n",
    "> - Steaming is ofter a shorter word having the **same root** meaning.\n",
    ">\n",
    "> - in other words it is sort of normalizing method to shorten the sentences, in order to increase redundancy and inefficiecy \n",
    "#### Example\n",
    "> \"I was taking a ride in the car.\"\n",
    ">\n",
    "> \"I Was riging in the car.\"\n",
    "#### Algorithm \n",
    "+ There are many steam algorithm. We will use the Porter Stemmer algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d3b25",
   "metadata": {
    "id": "cf8d3b25",
    "outputId": "5927151a-8fd0-4827-837e-d2928318f497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "is\n",
      "import\n",
      "to\n",
      "by\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# importing the PorterStemmer algorithm package\n",
    "from nltk.stem import PorterStemmer\n",
    "# importing the sentence and word tokenizer for tokenizing \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# create the object of PorterStemmer\n",
    "ps = PorterStemmer()  \n",
    "    \n",
    "# example sentencs\n",
    "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
    "\n",
    "# passing the example sentence in tokenizer\n",
    "words = word_tokenize(new_text)\n",
    "\n",
    "# printing the each word after steaming \n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53a9a7",
   "metadata": {
    "id": "7b53a9a7"
   },
   "source": [
    "### Parts of Speech Tagging\n",
    "Parts of speech this is another kind of tokenizer, and it tokentize by tagging the part of sppech of words in sentences, tagged as verb, adjective, noun , etc.\n",
    "\n",
    "In last chapter we have only used a sample of sentence but this also happen that the sentencs or sample data you want to use is different. \n",
    "\n",
    "May the data you want to use is the chat between a customer and seller,between teacher and student. The context, grammer in both cases it different. So to make the we should have a method to train the model on the grammer/context we want to test.\n",
    "\n",
    "We are using the \"PunktSentenceTokenizer\" this is capable to learn the tagging task according the given data because it is based on the unsupervised learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea916b",
   "metadata": {
    "id": "94ea916b",
    "outputId": "ed5157e9-a737-4af3-c14e-3262c3957b58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# importing state_union it is a corpus reader in PlainTextCorpusReader\n",
    "from nltk.corpus import state_union\n",
    "\n",
    "# importing the punktsentencetokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "# taking two type of text data set for training and testing \n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "\n",
    "# training the object of punktsentencetokenizer with train_text data set\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "\n",
    "# testing it on the sample data set\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "\n",
    "# function to print the pos with its words\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875150a",
   "metadata": {
    "id": "1875150a"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            # creating the chunk of the all adverb as chunkgram\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            chunked.draw()     \n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b864a",
   "metadata": {
    "id": "c83b864a",
    "outputId": "c85ce8cc-0e0b-446f-eb89-c179fd46666c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['(', 'dimension', 'films', ',', '\"', 'scream', '2', '\"', \"'\", 's', 'distributor', ',', 'has', 'asked', 'press', 'to', 'say', 'extremely', 'little', '--', 'if', 'anything', '--', 'about', 'the', 'film', \"'\", 's', 'twisty', 'plot', '.', 'that', \"'\", 's', 'no', 'easy', 'task', 'considering', 'the', 'wit', 'that', 'deserves', 'to', 'be', 'mentioned', 'here', ',', 'but', 'i', 'will', 'do', 'my', 'best', 'to', 'be', 'vague', '(', 'now', ',', 'there', \"'\", 's', 'a', 'first', ')', '.', ')', '\"', 'the', 'first', 'one', 'was', '[', 'good', ']', ',', 'but', 'all', 'the', 'rest', 'sucked', ',', '\"', 'said', 'a', 'cinematically', '-', 'savvy', 'teen', 'in', 'last', 'winter', \"'\", 's', 'wes', 'craven', 'thriller', '\"', 'scream', ',', '\"', 'her', 'statement', 'referring', 'to', 'the', 'films', 'of', 'the', '\"', 'nightmare', 'on', 'elm', 'street', '\"', 'series', 'but', 'really', 'putting', 'down', 'franchise', 'overkill', 'in', 'general', '.', 'the', 'comment', 'certainly', 'carries', 'clout', ':', 'for', 'every', 'truly', 'great', 'sequel', ',', 'there', 'appears', 'to', 'be', 'a', 'couple', 'of', 'duds', ',', 'making', 'one', 'wonder', 'if', 'writers', 'are', 'better', 'off', 'sticking', 'solely', 'with', 'fresh', 'ideas', '.', 'but', 'like', 'it', 'or', 'not', ',', 'along', 'comes', '\"', 'scream', '2', '.', '\"', 'and', 'believe', 'it', 'or', 'not', ',', 'it', \"'\", 's', 'a', 'doozy', '--', 'a', 'slick', ',', 'sinister', ',', 'madly', 'subversive', 'good', 'time', 'at', 'the', 'movies', ',', 'as', 'intent', 'on', 'sending', 'up', 'hollywood', \"'\", 's', 'sequel', 'syndrome', 'as', 'much', 'as', 'its', 'prequel', 'poked', 'fun', 'at', 'slasher', 'conventions', '.', '\"', 'scream', '2', '\"', 'is', 'definitely', 'that', 'rare', 'movie', 'thing', '--', 'a', 'follow', '-', 'up', 'that', 'can', 'stand', 'along', 'side', 'its', 'original', 'with', 'pride', '.', 'it', \"'\", 's', 'been', 'two', 'years', 'since', 'a', 'pair', 'of', 'overzealous', 'horror', 'movie', 'fans', 'clad', 'in', 'edvard', 'munch', '-', 'esque', 'get', '-', 'ups', 'carved', 'their', 'way', 'through', 'the', 'young', 'populace', 'of', 'woodsboro', ',', 'california', '.', 'those', 'surviving', 'the', 'ordeal', 'have', 'gotten', 'on', 'with', 'their', 'lives', '.', 'plucky', 'heroine', 'sidney', 'prescott', '(', 'neve', 'campbell', ')', 'is', 'a', 'drama', 'student', 'at', 'the', 'midwestern', 'windsor', 'college', ';', 'her', 'pop', 'culture', '-', 'whiz', 'pal', 'randy', 'meeks', '(', 'jamie', 'kennedy', ')', 'has', 'tagged', 'along', '.', 'trash', 'tabloid', 'reporter', 'gale', 'weathers', '(', 'courteney', 'cox', ')', 'has', 'written', 'a', 'best', '-', 'seller', 'based', 'on', 'their', 'ordeal', ',', 'the', 'basis', 'of', 'which', 'has', 'been', 'turned', 'into', 'a', '(', 'very', 'bad', ')', 'movie', 'called', '\"', 'stab', '.', '\"', 'and', 'dewey', 'riley', '(', 'david', 'arquette', ')', ',', 'still', 'suffering', 'from', 'wounds', 'inflicted', 'during', '\"', 'scream', ',', '\"', 'has', 'left', 'his', 'job', 'as', 'a', 'police', 'officer', 'for', 'a', 'while', '.', 'life', 'is', 'tranquil', '.', '.', '.', 'at', 'least', 'for', 'a', 'while', '.', 'several', 'sudden', 'murders', 'bring', 'sidney', ',', 'randy', ',', 'gale', 'and', 'dewey', 'together', 'again', ',', 'but', 'with', 'suspicious', 'eyes', 'cast', 'on', 'each', 'other', 'and', 'most', 'of', 'those', 'in', 'their', 'surroundings', '--', 'if', 'these', 'four', 'people', 'learned', 'anything', 'from', 'the', 'past', ',', 'it', \"'\", 's', 'to', 'trust', 'no', 'one', '.', 'thus', ',', 'the', 'possible', 'victim', '/', 'potential', 'killer', 'list', 'includes', 'said', 'quartet', ',', 'as', 'well', 'as', ':', 'derek', '(', 'jerry', 'o', \"'\", 'connell', ')', ',', 'sidney', \"'\", 's', 'new', 'beau', ';', 'cici', '(', 'sara', 'michelle', 'gellar', ')', ',', 'a', 'chatty', 'sorority', 'gal', ';', 'joel', '(', 'duane', 'martin', ')', ',', 'gale', \"'\", 's', 'cameraman', 'who', \"'\", 's', 'not', 'too', 'thrilled', 'with', 'her', 'blood', '-', 'soaked', 'past', ';', 'hallie', '(', 'elise', 'neal', ')', ',', 'sidney', \"'\", 's', 'sassy', 'roommate', ';', 'debbie', '(', 'laurie', 'metcalf', ')', ',', 'a', 'local', 'reporter', 'who', 'gives', 'gale', 'some', 'not', '-', 'too', '-', 'friendly', 'competition', ';', 'and', 'mickey', '(', 'timothy', 'olyphant', ')', ',', 'randy', \"'\", 's', 'good', 'friend', 'and', 'fellow', 'film', 'student', '.', 'cotton', 'weary', '(', 'liev', 'schreiber', ')', ',', 'the', 'man', 'sidney', 'wrongly', 'accused', 'of', 'her', 'mother', \"'\", 's', 'murder', 'in', 'scream', ',', 'also', 'shows', 'up', 'on', 'campus', '--', 'but', 'why', '?', 'like', 'the', 'first', 'scream', ',', 'craven', 'and', 'screenwriter', 'kevin', 'williamson', 'inaugurate', 'things', 'with', 'a', 'bang', '.', 'this', 'time', ',', 'it', \"'\", 's', 'an', 'extended', 'sequence', 'that', 'finds', 'an', 'african', '-', 'american', 'couple', '(', 'jada', 'pinkett', 'and', 'omar', 'epps', ')', 'attending', 'a', 'sneak', 'preview', 'of', '\"', 'stab', ',', '\"', 'making', 'sly', 'references', 'about', 'everything', 'from', 'sandra', 'bullock', 'to', 'the', 'lunchmeat', 'non', '-', 'roles', 'of', 'blacks', 'in', 'slasher', 'cinema', '.', 'before', 'tragedy', 'ensues', 'at', 'the', 'screening', ',', 'the', 'movie', '-', 'within', '-', 'a', '-', 'movie', 'setup', 'allows', 'for', 'plenty', 'of', 'jokey', 'moments', ',', 'including', 'a', 'scene', 'from', '\"', 'stab', '\"', 'which', 'turns', 'scream', \"'\", 's', 'now', '-', 'famous', 'drew', 'barrymore', 'prologue', 'on', 'its', 'ear', '.', '(', '\"', 'b', '----', ',', 'hang', 'up', 'the', 'phone', 'and', 'star', '-', '69', 'his', 'a', '--', '!', '\"', 'pinkett', 'yells', 'at', 'the', 'screen', '.', ')', 'it', \"'\", 's', 'funny', ',', 'creepy', 'stuff', '(', 'i', 'don', \"'\", 't', 'think', 'i', \"'\", 'll', 'ever', 'feel', 'safe', 'in', 'a', 'movie', 'theater', 'again', ')', 'that', 'effectively', 'foreshadows', 'the', 'ratio', 'of', 'smart', 'scares', 'and', 'spoofy', 'laughs', 'down', 'the', 'road', '.', 'another', 'flawless', 'example', 'of', 'this', 'is', 'a', 'riotous', 'cell', 'phone', '-', 'assault', 'of', 'the', 'killer', 'on', 'randy', ',', 'dewey', 'and', 'gale', 'in', 'broad', 'daylight', ';', 'it', 'builds', 'comedically', 'and', 'ends', 'with', 'genuine', 'terror', 'because', 'of', 'the', 'emotional', 'investment', 'made', 'on', 'these', 'delicious', 'characters', '.', 'there', 'are', 'other', 'superb', 'set', 'pieces', ',', 'but', 'explaining', 'them', 'risks', 'ruining', 'their', 'effect', '.', 'there', 'are', 'more', 'players', 'this', 'time', 'around', 'as', 'to', 'provide', 'both', 'a', 'higher', 'number', 'suspects', 'and', 'a', 'staggering', 'body', 'count', '.', 'it', \"'\", 's', 'something', 'of', 'a', 'trade', '-', 'in', '--', 'the', 'new', 'characters', 'are', 'too', 'large', 'in', 'number', 'to', 'be', 'as', 'fully', '-', 'developed', 'as', 'the', 'original', '\"', 'scream', '\"', 'gang', ',', 'but', 'this', 'movie', 'packs', 'in', 'knowing', 'performances', '(', 'especially', 'by', 'kennedy', 'and', 'cox', ')', 'and', 'more', 'death', 'for', 'your', 'dollar', '.', 'it', 'can', 'be', 'argued', 'that', ',', 'while', 'the', 'original', 'might', 'outclass', 'it', 'by', 'a', 'tad', ',', 'scream', '2', 'is', 'both', 'scarier', 'and', 'funnier', '.', 'it', 'certainly', 'doesn', \"'\", 't', 'skimp', 'with', 'shocks', '--', 'a', 'major', 'one', 'being', 'the', 'killing', 'of', 'one', 'cast', 'member', 'very', 'near', 'and', 'dear', 'to', 'my', 'heart', '(', 'i', 'actually', 'questioned', 'craven', 'and', 'williamson', \"'\", 's', 'judgment', 'here', ',', 'considering', 'how', 'much', 'this', 'person', 'has', 'added', 'to', 'these', 'films', ')', '.', 'sometimes', 'you', 'wish', 'that', 'more', 'could', 'have', 'been', 'done', 'with', 'stab', '--', 'the', 'two', 'scenes', 'we', \"'\", 're', 'shown', 'are', 'dead', '-', 'on', '--', 'but', 'for', 'film', 'fans', ',', 'there', 'are', 'plenty', 'of', 'subtle', 'nods', 'to', 'movies', 'like', '\"', 'the', 'usual', 'suspects', ',', '\"', '\"', 'aliens', '\"', 'and', '\"', 'the', 'empire', 'strikes', 'back', '.', '\"', 'still', ',', '\"', 'scream', '2', '\"', 'does', 'so', 'many', 'things', 'right', ',', 'it', \"'\", 's', 'petty', 'to', 'quibble', '.', 'its', 'penchant', 'for', 'parody', 'is', 'irresistible', ',', 'discussions', 'of', 'the', 'merits', 'of', 'film', 'sequels', 'and', 'all', '.', 'the', 'attractive', ',', 'solid', 'cast', 'is', 'a', 'definite', 'bonus', '.', 'and', 'the', 'denouement', '(', 'while', 'not', 'as', 'shocking', 'as', '\"', 'scream', '\"', \"'\", 's', ')', 'is', 'priceless', ';', 'the', 'film', \"'\", 's', 'final', 'revelation', 'incorporates', 'bits', 'of', 'a', 'particular', 'horror', 'film', 'prominently', 'referred', 'to', 'in', '\"', 'scream', '\"', 'with', 'one', 'of', 'the', 'best', 'bad', 'guy', 'motives', 'ever', 'put', 'on', 'paper', '.', 'all', 'this', 'and', 'a', '\"', 'scream', '3', '\"', 'is', 'in', 'the', 'works', '.', 'knowing', 'the', 'series', \"'\", 'tendency', 'for', 'self', '-', 'reference', ',', 'perhaps', 'one', 'of', 'its', 'characters', 'will', 'again', 'discuss', 'the', 'concept', 'of', 'sequel', 'justice', '.', 'mentioning', '\"', 'scream', '2', '\"', \"'\", 'll', 'help', 'them', 'in', 'their', 'argument', '.'], 'pos')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c0ef2f",
   "metadata": {
    "id": "36c0ef2f"
   },
   "outputs": [],
   "source": [
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "for i in movie_reviews.fileids():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a149b35",
   "metadata": {
    "id": "0a149b35"
   },
   "outputs": [],
   "source": [
    "random.shuffle(documents)\n",
    "\n",
    "print(documents[1])\n",
    "\n",
    "# all_words = []\n",
    "# for w in movie_reviews.words():\n",
    "#     all_words.append(w.lower())\n",
    "\n",
    "# all_words = nltk.FreqDist(all_words)\n",
    "# print(all_words.most_common(15))\n",
    "# print(all_words[\"stupid\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
