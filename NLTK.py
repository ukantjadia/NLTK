# -*- coding: utf-8 -*-
"""NLTK - intership.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CDEZlu-chW1Wh2dH1nIP3MIMLkFm3QB9

## What is out main Aim ?
> As humans we just want to use machines(computer) in order to redure the human effort and here our Aim for NLTk is to use the Natural language Processing to do judge/analysis of some writtend dwon text, in  CSE anaology we are using NLP to revice some sort of ouput/result of what text means and implies.
"""

import nltk
nltk.download()

from nltk.tokenize import sent_tokenize, word_tokenize 
EXAMPLE_TEXT = "Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard."
# print(sent_tokenize(EXAMPLE_TEXT))
print(word_tokenize(EXAMPLE_TEXT))

"""For human the task of analysis/judge is done quite easily but it is impossible for computer to undertstand something which is not in in 0 or 1 (binary/macine code) 


Bammer : humans are also not good at this task. The human memory is work as electrical signal in the form of neural groups which fires in the patterns.


As I earlier said computer are created to just reduce the human effort and in order to build there efficiency and accuracy they are limited to the some extent, here limitation means they are limited with the way they perform there task(the procedure to done something), it include .
computers only work in binary 0,1.

There are some way to convert words to values, in numbers or in signal patterns.

## Pre-Processing 
> The process of converting data to something a computer can understand is reffered as pre-processing.(Big Task)

- here we are going to filter out useless data, it is a major form of Pre-processing.
- and useless words(data), are referred as 'stop words'.

Example containing stop(useless) words : 
example_sent = "This is a sample sentence, showing off the stop words filtration."

from above written example we can recognize that some words carry more meaning than other words. 
 
In real world example is, when we speak 'umm' or 'uhh'. This kind of words means nothing to our real meaning of what we say.
we call this kind of words "stop words".



As humans we don't have much problem to have them in our text effort lessly but machine can't do the same, it will increase the space and process time for treating/converting them as meaning words.



To see list what we considered as stop word 
use following code
"""

from nltk.corpus import stopwords
print(stopwords.words('english'))

from nltk.corpus import stopwords as sp
from nltk.tokenize import word_tokenize as wt

# taking the old example_sent
example_sent = "This is a sample sentence, showing off the stop words filtration."

# taking the stored stop words
stop_words = set(sp.words('english'))

# tokenizing the example_sentence for filtering
word_token = wt(example_sent)

# taking out the words which are not in the sto_word set
filtered_sentence = [ w for w in word_token if not w in stop_words]
print(filtered_sentence)

"""## Steaming 
> - Steaming is technique to extract the base form of the words by **removing affixes** from the resulting words.
>
> - Steaming is ofter a shorter word having the **same root** meaning.
>
> - in other words it is sort of normalizing method to shorten the sentences, in order to increase redundancy and inefficiecy 
#### Example
> "I was taking a ride in the car."
>
> "I Was riging in the car."
#### Algorithm 
+ There are many steam algorithm. We will use the Porter Stemmer algorithm. 
"""

# importing the PorterStemmer algorithm package
from nltk.stem import PorterStemmer
# importing the sentence and word tokenizer for tokenizing 
from nltk.tokenize import sent_tokenize, word_tokenize

# create the object of PorterStemmer
ps = PorterStemmer()  
    
# example sentencs
new_text = "It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once."

# passing the example sentence in tokenizer
words = word_tokenize(new_text)

# printing the each word after steaming 
for w in words:
    print(ps.stem(w))

"""### Parts of Speech Tagging
Parts of speech this is another kind of tokenizer, and it tokentize by tagging the part of sppech of words in sentences, tagged as verb, adjective, noun , etc.

In last chapter we have only used a sample of sentence but this also happen that the sentencs or sample data you want to use is different. 

May the data you want to use is the chat between a customer and seller,between teacher and student. The context, grammer in both cases it different. So to make the we should have a method to train the model on the grammer/context we want to test.

We are using the "PunktSentenceTokenizer" this is capable to learn the tagging task according the given data because it is based on the unsupervised learning.


"""

import nltk
# importing state_union it is a corpus reader in PlainTextCorpusReader
from nltk.corpus import state_union

# importing the punktsentencetokenizer
from nltk.tokenize import PunktSentenceTokenizer


# taking two type of text data set for training and testing 
train_text = state_union.raw("2005-GWBush.txt")
sample_text = state_union.raw("2006-GWBush.txt")


# training the object of punktsentencetokenizer with train_text data set
custom_sent_tokenizer = PunktSentenceTokenizer(train_text)


# testing it on the sample data set
tokenized = custom_sent_tokenizer.tokenize(sample_text)


# function to print the pos with its words
def process_content():
    try:
        for i in tokenized[:5]:
            words = nltk.word_tokenize(i)
            tagged = nltk.pos_tag(words)
            print(tagged)

    except Exception as e:
        print(str(e))


process_content()

import nltk
from nltk.corpus import state_union
from nltk.tokenize import PunktSentenceTokenizer

train_text = state_union.raw("2005-GWBush.txt")
sample_text = state_union.raw("2006-GWBush.txt")

custom_sent_tokenizer = PunktSentenceTokenizer(train_text)

tokenized = custom_sent_tokenizer.tokenize(sample_text)

def process_content():
    try:
        for i in tokenized:
            words = nltk.word_tokenize(i)
            tagged = nltk.pos_tag(words)
            # creating the chunk of the all adverb as chunkgram
            chunkGram = r"""Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}"""
            chunkParser = nltk.RegexpParser(chunkGram)
            chunked = chunkParser.parse(tagged)
            chunked.draw()     

    except Exception as e:
        print(str(e))

process_content()

import nltk
import random
from nltk.corpus import movie_reviews

documents = [(list(movie_reviews.words(fileid)), category)
             for category in movie_reviews.categories()
             for fileid in movie_reviews.fileids(category)]

for i in movie_reviews.fileids():
    print(i)

random.shuffle(documents)

print(documents[1])

# all_words = []
# for w in movie_reviews.words():
#     all_words.append(w.lower())

# all_words = nltk.FreqDist(all_words)
# print(all_words.most_common(15))
# print(all_words["stupid"])



